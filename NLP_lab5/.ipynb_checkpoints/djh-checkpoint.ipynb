{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Bidirectional, Input, Dense, Activation, Embedding, Dropout, TimeDistributed, GRU, Add, Lambda\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from data_helper import load_data\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"\" # uncomment this line, if you use cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "dropout_rate = 0.2\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "embedding_dim = 100\n",
    "\n",
    "# gru parameters\n",
    "hidden_dim = 100\n",
    "num_encoder_layer = 2\n",
    "num_decoder_layer = 1\n",
    "\n",
    "# attention parameters\n",
    "# None donates that no self-attenti0n mechanism is used\n",
    "# 'dot' indicated the self-attention mechanism presented in the tutorial\n",
    "attention_type = 'dot'  # [None, 'dot', 'multiplicative', 'additive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_predict(seq2seq_model, encoder_input, decoder_sequence_length, sos_idx):\n",
    "    # because we do not have the truth decoder input,\n",
    "    # we need to use the decoder prediction as its input\n",
    "    decoder_input = np.zeros(\n",
    "        shape=(len(encoder_input), decoder_sequence_length))\n",
    "    decoder_input[:, 0] = sos_idx\n",
    "    for i in range(1, decoder_sequence_length):\n",
    "        output = seq2seq_model.predict(\n",
    "            [encoder_input, decoder_input], batch_size=batch_size).argmax(axis=2)\n",
    "        decoder_input[:, i] = output[:, i-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_sentence(x, idx2word):\n",
    "    s = []\n",
    "    for idx in x:\n",
    "        word = idx2word[idx]\n",
    "        if word == '<sos>':\n",
    "            continue\n",
    "        elif word == '<eos>':\n",
    "            break\n",
    "        elif word == '<pad>':\n",
    "            break\n",
    "        s.append(word)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    \"\"\"\n",
    "    Calculate BLEU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_data, model, vocabulary):\n",
    "        self.test_data = test_data\n",
    "        self.model = model\n",
    "        self.vocabulary = vocabulary\n",
    "        self.idx2word = dict()\n",
    "        for k, v in self.vocabulary.items():\n",
    "            self.idx2word[v] = k\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        [encoder_input, decoder_input, decoder_target] = self.test_data\n",
    "        decoder_output = seq2seq_predict(\n",
    "            self.model, encoder_input, decoder_input_train.shape[1], vocabulary['<sos>'])\n",
    "        bleu, results = self.evaluate_bleu(decoder_target, decoder_output)\n",
    "        results.sort(reverse=True)\n",
    "        print('Validation Set BLEU: %f' % (bleu))\n",
    "        print('Top | BLEU | %s | %s' %\n",
    "              ('target'.ljust(20), 'output'.ljust(20)))\n",
    "        indices = list(range(len(results)))\n",
    "        candidate_indices = list()\n",
    "        candidate_indices.extend(indices[0:3])\n",
    "        step = len(indices)//10\n",
    "        if step > 0:\n",
    "            candidate_indices.extend(indices[2+step::step])\n",
    "        if indices[-1] != candidate_indices[-1]:\n",
    "            candidate_indices.append(indices[-1])\n",
    "        for i in candidate_indices:\n",
    "            r = results[i]\n",
    "            print('%-4d|%.4f| %s | %s' %\n",
    "                  (i, r[0], ' '.join(r[1]), ' '.join(r[2])))\n",
    "\n",
    "    def evaluate_bleu(self, target, output):\n",
    "        N = target.shape[0]\n",
    "        sum_bleu = 0.0\n",
    "        results = []\n",
    "        for i in range(N):\n",
    "            t = recover_sentence(target[i], self.idx2word)\n",
    "            o = recover_sentence(output[i], self.idx2word)\n",
    "            bleu = nltk.translate.bleu_score.sentence_bleu([t], o)\n",
    "            sum_bleu += bleu\n",
    "            results.append((bleu, t, o))\n",
    "        return sum_bleu / N, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "encoder_input_train.shape (12244, 10)\n",
      "decoder_input_train.shape (12244, 10)\n",
      "Vocab Size 3243\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 10, 100)      324300      input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 10, 100)      0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 10, 200)      121200      dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 10, 200)      0           bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional [(None, 10, 200), (N 181200      dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 10, 100)      0           embedding_9[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 200)          0           bidirectional_14[0][1]           \n",
      "                                                                 bidirectional_14[0][2]           \n",
      "__________________________________________________________________________________________________\n",
      "gru_17 (GRU)                    (None, 10, 200)      181200      dropout_23[0][0]                 \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 10, 10)       0           gru_17[0][0]                     \n",
      "                                                                 bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 10)       0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 10, 200)      0           activation_1[0][0]               \n",
      "                                                                 bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 10, 400)      0           gru_17[0][0]                     \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 10, 3243)     1300443     concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,108,343\n",
      "Trainable params: 2,108,343\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Traning Model...\n",
      "Epoch 1/20\n",
      "192/192 - 67s - loss: 2.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DU183\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\DU183\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\DU183\\.conda\\envs\\tensorflow-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set BLEU: 0.004873\n",
      "Top | BLEU | target               | output              \n",
      "0   |1.0000| she is a <unk> . | she is a <unk> .\n",
      "1   |1.0000| i m <unk> . | i m <unk> .\n",
      "2   |1.0000| i m <unk> . | i m <unk> .\n",
      "124 |0.0000| i m not a fan . | i m not to to you .\n",
      "246 |0.0000| i m finicky . | i m a .\n",
      "368 |0.0000| i m too drunk . | i m very .\n",
      "490 |0.0000| he is in <unk> . | we re <unk> .\n",
      "612 |0.0000| you re no friend of mine . | you re not <unk> .\n",
      "734 |0.0000| you re part of the problem . | you re not .\n",
      "856 |0.0000| i am working . | i m not <unk> .\n",
      "978 |0.0000| they re late as usual . | he is a <unk> <unk> .\n",
      "1100|0.0000| we re closed on mondays . | you re not .\n",
      "1222|0.0000| i m going to miss you a lot . | you re not .\n",
      "1224|0.0000| he is cool isn t he ? | you re a .\n",
      "Epoch 2/20\n",
      "192/192 - 58s - loss: 1.7503\n",
      "Validation Set BLEU: 0.027116\n",
      "Top | BLEU | target               | output              \n",
      "0   |1.0000| you re <unk> . | you re <unk> .\n",
      "1   |1.0000| we re <unk> . | we re <unk> .\n",
      "2   |1.0000| we re <unk> . | we re <unk> .\n",
      "124 |0.0000| i m not dating tom . | i m not not <unk> .\n",
      "246 |0.0000| i m sorry my father is out . | i m sorry to see you .\n",
      "368 |0.0000| we re <unk> . | we re going to the <unk> .\n",
      "490 |0.0000| he s too busy . | he s very <unk> .\n",
      "612 |0.0000| i m freezing cold . | i m <unk> .\n",
      "734 |0.0000| i am going to get dressed . | i m going to <unk> .\n",
      "856 |0.0000| you re the one that went crazy . | you re not good as you .\n",
      "978 |0.0000| i am <unk> my teeth . | i m going to <unk> .\n",
      "1100|0.0000| she s strong willed . | she is <unk> .\n",
      "1222|0.0000| he is pleased with his work . | she s <unk> .\n",
      "1224|0.0000| they aren t alone . | you re not not\n",
      "Epoch 3/20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Loading data')\n",
    "    encoder_input_train, decoder_input_train, decoder_target_train, \\\n",
    "        encoder_input_valid, decoder_input_valid, decoder_target_valid, vocabulary = load_data(\n",
    "            'translation')\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    print('encoder_input_train.shape', encoder_input_train.shape) # （12244，10）\n",
    "    print('decoder_input_train.shape', decoder_input_train.shape) # （12244，10）\n",
    "    print('Vocab Size', vocab_size) # 3243\n",
    "\n",
    "    num_training_data = encoder_input_train.shape[0]\n",
    "    encoder_sequence_length = encoder_input_train.shape[1]\n",
    "    decoder_sequence_length = decoder_input_train.shape[1]\n",
    "\n",
    "    # encoder_input -> [batch_size, encoder_sequence_length]\n",
    "    # decoder_input -> [batch_size, decoder_sequence_length]\n",
    "    encoder_input = Input(shape=(encoder_sequence_length,), dtype='int32')\n",
    "    decoder_input = Input(shape=(decoder_sequence_length,), dtype='int32')\n",
    "\n",
    "    # the encoder and decoder share the same embedding layer\n",
    "    emb_layer = Embedding(input_dim=vocab_size,\n",
    "                          output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "    ################\n",
    "    # ENCODER PART #\n",
    "    ################\n",
    "\n",
    "    # embedding -> [batch_size, sequence_length, embedding_dim]\n",
    "    ### YOUR CODE HERE ###\n",
    "    #encoder_input_embed = None\n",
    "    encoder_input_embed = emb_layer(encoder_input)\n",
    "\n",
    "    # dropout at embedding layer\n",
    "    ### YOUR CODE HERE ###\n",
    "    encoder_input_droped = Dropout(dropout_rate)(encoder_input_embed)\n",
    "\n",
    "    # add multiple Bidirectional GRU layers here,\n",
    "    # set units=hidden_dim, return_sequences=True at the previous layers\n",
    "    # set units=hidden_dim, return_sequences=True, return_state=True at the last layer\n",
    "    # please read https://keras.io/layers/recurrent/\n",
    "    # output:\n",
    "    #     if return_sequences==True:\n",
    "    #         gru_output -> [batch_size, sequence_length, 2*hidden_dim]\n",
    "    #     if return_sequences==True and return_state=True:\n",
    "    #         gru_output -> [batch_size, sequence_length, 2*hidden_dim], [batch_size, hidden_dim], [batch_size, hidden_dim]\n",
    "\n",
    "    encoder_inputs = [encoder_input_droped]\n",
    "    for i in range(0, num_encoder_layer-1):\n",
    "        # N − 1 layer(s) of Bidirectional GRU, which return(s) sequences only, i.e., set return_sequences=True, unroll=True.\n",
    "        ### YOUR CODE HERE ###\n",
    "        encoder_output = Bidirectional(GRU(units=hidden_dim,return_sequences=True, unroll=True))(encoder_inputs[i])\n",
    "\n",
    "        # Dropout layers between GRU layers if applicable.\n",
    "        ### YOUR CODE HERE ###\n",
    "        encoder_output_droped = Dropout(dropout_rate)(encoder_output)\n",
    "\n",
    "        encoder_inputs.append(encoder_output_droped)\n",
    "    # 1 layer of Bidirectional GRU, which returns sequences and the last state.\n",
    "    encoder_output, encoder_last_h, encoder_last_hr = Bidirectional(GRU(units=hidden_dim,\n",
    "                                                                        return_sequences=True, \n",
    "                                                                        return_state=True, \n",
    "                                                                        unroll=True))(encoder_inputs[-1])\n",
    "\n",
    "    ################\n",
    "    # DECODER PART #\n",
    "    ################\n",
    "\n",
    "    # embedding -> [batch_size, sequence_length, embedding_dim]\n",
    "    ### YOUR CODE HERE ###\n",
    "    decoder_input_embed = emb_layer(decoder_input)\n",
    "\n",
    "    # dropout at embedding layer\n",
    "    ### YOUR CODE HERE ###\n",
    "    decoder_input_droped = Dropout(dropout_rate)(decoder_input_embed)\n",
    "\n",
    "    # add multiple Unidirectional GRU layers here,\n",
    "    # set units=2*hidden_dim, return_sequences=True\n",
    "    # set initial_state=encoder_hidden_state at the first layer\n",
    "    # please read https://keras.io/layers/recurrent/\n",
    "    # output:\n",
    "    # gru_output -> [batch_size, sequence_length, 2*hidden_dim]\n",
    "\n",
    "    # 1 layer of Unidirectional GRU, whose input is the output of the encoder’s last layer\n",
    "    # and initial hidden state is the encoder’s last state, i.e., initial_state=concatenate([encoder_last_h, encoder_last_hr], axis=1)\n",
    "    ### YOUR CODE HERE ###\n",
    "    initial_state=concatenate([encoder_last_h, encoder_last_hr], axis=1)\n",
    "    decoder_output = GRU(units=2*hidden_dim, return_sequences=True)(decoder_input_droped,initial_state=concatenate([encoder_last_h, encoder_last_hr], axis=1))\n",
    "    decoder_outputs = [decoder_output]\n",
    "    # M − 1 layer(s) of Unidirectional GRU.\n",
    "    # Dropout layers between GRU layers if applicable (optional).\n",
    "    for i in range(1, num_decoder_layer):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass\n",
    "\n",
    "    # simple seq2seq without any attention mechanism\n",
    "    if attention_type is None:\n",
    "        # 1 layer of Dense layer with softmax activation wrapped by TimeDistributed.\n",
    "        output = TimeDistributed(\n",
    "            Dense(units=vocab_size, activation='softmax'))(decoder_outputs[-1])\n",
    "    else:\n",
    "        # dot-product attention\n",
    "        # weight_{i,j} = softmax(\\sum_k {decoder_output_{i,k} * encoder_output_{j,k}})\n",
    "        if attention_type == 'dot':\n",
    "            ### YOUR CODE HERE ###\n",
    "            #weight = None\n",
    "            weight = Activation('softmax')(\n",
    "                dot([decoder_outputs[-1], encoder_output], axes=[2, 2]))\n",
    "            \n",
    "        # multiplicative attention\n",
    "        # weight_{i,j} = softmax(\\sum_k {decoder_output_{i,k} * (W encoder_output_{j,k})})\n",
    "        elif attention_type == 'multiplicative':\n",
    "            encoder_output_maped = Dense(units=2*hidden_dim, bias=False)(encoder_output)\n",
    "            weight = Activation('softmax')(\n",
    "                dot([decoder_outputs[-1], encoder_output_maped], axes=[2, 2]))\n",
    "            \n",
    "        # additive attention\n",
    "        # weight_{i, j} = softmax(\\sum_k {V tanh(W1 decoder_output_{i,k} + W2 encoder_output_{j,k})})\n",
    "        elif attention_type == 'additive':\n",
    "            # You may need the help of the Lambda wrapper(https://keras.io/layers/core/#lambda)\n",
    "            # the K.expand_dims function and the K.squeeze function(https://keras.io/backend/#backend-functions\n",
    "            decoder_output_maped = Dense(units=2*hidden_dim, bias=False)(decoder_output)\n",
    "            encoder_output_maped = Dense(units=2*hidden_dim, bias=False)(encoder_output)\n",
    "            tanh_result = Activation('tanh')(Add()([\n",
    "                Lambda(lambda x: K.expand_dims(x, axis=2))(decoder_output_maped), \n",
    "                Lambda(lambda x: K.expand_dims(x, axis=1))(encoder_output_maped)]))\n",
    "            weight = Activation('softmax')(\n",
    "                Lambda(lambda x: K.squeeze(x, axis=3))(\n",
    "                    Dense(units=1, bias=False)(tanh_result)))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        attention = dot([weight, encoder_output], axes=[2, 1])\n",
    "        output = TimeDistributed(Dense(units=vocab_size, activation='softmax'))(\n",
    "            concatenate([decoder_outputs[-1], attention], axis=2))\n",
    "\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "\n",
    "    adam = Adam()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Traning Model...\")\n",
    "    history = model.fit([encoder_input_train, decoder_input_train], np.expand_dims(decoder_target_train, axis=2), \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,\n",
    "                        verbose=1 if os.name == 'posix' else 2, \n",
    "                        callbacks=[TestCallback((encoder_input_valid, decoder_input_valid, decoder_target_valid), model, vocabulary)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
