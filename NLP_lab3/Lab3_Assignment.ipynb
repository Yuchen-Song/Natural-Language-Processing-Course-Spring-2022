{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fleet-evidence",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 3 Exploring Word Vector\n",
    "### SHINE-MING WU SCHOOL OF INTELLIGENT ENGINEERING - $\\color{blue}{Spring 2022}$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-nashville",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-raleigh",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prerequisties\n",
    "- You need to install Gensim with pip by typing the following on your command line:\n",
    "\n",
    "    `pip install --upgrade gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-officer",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <font color=\"blue\">Assignment 1.</font> Train a word2vec model using Gensim [code]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-witch",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Here, we will be using Text8 corpus, and train a word2vec model using Gensim.\n",
    "\n",
    "This module is an API for downloading, getting information and loading corpus/datasets, and indicate that how to train Word2Vector model.\n",
    "\n",
    "[Downloader API for gensim](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepting-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here.\n",
    "# dataset =   # load dataset as iterable\n",
    "# model =   # train w2v model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-giving",
   "metadata": {},
   "source": [
    "Learn more about Word2Vector model, you can refer to the blog (note that, this is not the latest version):\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "or the official documentation (latest):\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-heaven",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <font color=\"blue\">Assignment 2.</font> Find top 10 similar words [code]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-semester",
   "metadata": {},
   "source": [
    "### 2.1 Use the Word2Vec model obtained in Assignment 1, find top 10 similar words of the following words:\n",
    "\n",
    "`cat apple student happy quickly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-comedy",
   "metadata": {},
   "source": [
    "### 2.2 Find top 10 similar words of the following words, in contrast to 2.1, here you will use the model produced by GloVe.\n",
    "\n",
    "`computer cat apple student happy`\n",
    "\n",
    "Glove algorithm, which utilizes the benefit of counts. If you'd like more details, challenge yourself and try reading [GloVe's original paper](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "Now, Run the following cells to load the GloVe vectors into memory. Note: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "intense-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = wv_from_bin.most_similar('computer', topn=10)  # get other similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-policy",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <font color=\"blue\">Assignment 3.</font> Analogies with Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-beaver",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1 Analogies with Word Vectors [code]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-massachusetts",
   "metadata": {},
   "source": [
    "Word vectors have been shown to sometimes exhibit the ability to solve analogies.\n",
    "\n",
    "As an example, for the analogy \"man : grandfather :: woman : x\" (read: man is to grandfather as woman is to x)\n",
    "\n",
    "the [GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) show you how to use word vectors to find x using the `most_similar` function. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list (while omitting the input words, which are often the most similar). Please refer to the [GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) and answer the following analogy tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-crystal",
   "metadata": {},
   "source": [
    "man : king :: woman : ?\n",
    "\n",
    "berlin : germany :: bangkok : ?\n",
    "\n",
    "helsinki : finland :: paris : ? \n",
    "\n",
    "argentina : spanish :: egypt : ?\n",
    "\n",
    "bear : cub :: deer : ? Answer : ? \n",
    "\n",
    "bee: hive :: bat : ? Answer: ?\n",
    "\n",
    "broccoli : green :: milk : ? \n",
    "\n",
    "groom : bride :: husband : ? \n",
    "\n",
    "son : daughter :: nephew : ? \n",
    "\n",
    "where you can use the Glove model in **Assignment 2.2**, i.e.,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here.\n",
    "# x:y :: a:b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-supplier",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.2 Find a bad example of word analogy [code + written] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-serbia",
   "metadata": {},
   "source": [
    "Find an example of analogy that does not hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bad example\n",
    "import pprint\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['biased', 'unable'],negative=['able']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-cyprus",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <font color=\"blue\">Assignment 4.</font> Implement plot_embeddings [code]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-assurance",
   "metadata": {
    "tags": []
   },
   "source": [
    "Visualize word embeddings. You are required to use TSNE/PCA methods to reduce the high-dimensional word vectors to two-dimensional plots and plot them on a graph. For this example, you may find it useful to adapt [this code](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#visualising-word-embeddings). Visualize these words: \n",
    "\n",
    "`he\n",
    "she\n",
    "his\n",
    "her\n",
    "female\n",
    "male\n",
    "woman\n",
    "man\n",
    "women\n",
    "men\n",
    "father\n",
    "mother\n",
    "sister\n",
    "brother\n",
    "boy\n",
    "girl\n",
    "housekeeper\n",
    "mechanic\n",
    "carpenter\n",
    "dancer\n",
    "engineer\n",
    "chief\n",
    "lawyer\n",
    "developer\n",
    "physician\n",
    "driver\n",
    "librarian\n",
    "nurse\n",
    "doctor\n",
    "cashier\n",
    "secretary\n",
    "prince\n",
    "princess`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-spirit",
   "metadata": {},
   "source": [
    "Before visualize word embedding, you can see this step-by-step tutorial:\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-discovery",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <font color=\"blue\">Assignment 5.</font> Guided Analysis of Bias in Word Vectors [written]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-latvia",
   "metadata": {},
   "source": [
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"girl\" and \"toy\" and most dissimilar to \"boy\", and (b) which terms are most similar to \"boy\" and \"toy\" and most dissimilar to \"girl\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias.\n",
    "\n",
    "**Note that**, use words to answer rather than code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "import pprint\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['girl', 'toy'], negative=['boy']))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['boy', 'toy'], negative=['girl']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-canberra",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Assignment 6.</font> Derivative with respect to the word embedding [written]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-commission",
   "metadata": {},
   "source": [
    "Given the objective functions of the skip-gram model using negative sampling techniques,  \n",
    "$$J = \\frac{1}{T} \\sum^{T}_{t=1} J_t$$\n",
    "$$J_t = \\log \\sigma(u^T_o v_c) + \\sum^{k}_{j=1} [\\log \\sigma(-u^T_j v_c)]$$\n",
    "Calculate $\\frac{\\partial J_t}{\\partial u_o}$ and $\\frac{\\partial J_t}{\\partial v_c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-henry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
